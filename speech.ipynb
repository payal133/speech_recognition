{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import gfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TRAIN='train'\n",
    "PATH_TEST='test'\n",
    "BATCH_SIZE=100\n",
    "ITERATIONS=500\n",
    "ITERATIONS_TEST=10\n",
    "EVAL_EVERY=5\n",
    "HEIGHT=20\n",
    "WIDTH=44\n",
    "NUM_LABELS=0\n",
    "LEARNING_RATE=1E-4\n",
    "LOGDIR='log/'\n",
    "TEST_LOGDIR='log_test/'\n",
    "LABEL_TO_INDEX_MAP={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(path):\n",
    "    labels=os.listdir(path)\n",
    "    index=0\n",
    "    for label in labels:\n",
    "        LABEL_TO_INDEX_MAP[label]=index\n",
    "        index+=1\n",
    "        \n",
    "    global NUM_LABELS\n",
    "    NUM_LABELS =len(LABEL_TO_INDEX_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(label):\n",
    "    encoding=[0]*len(LABEL_TO_INDEX_MAP)\n",
    "    encoding[LABEL_TO_INDEX_MAP[label]]=1\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfcc(wav_path,PAD_WIDTH=WIDTH):\n",
    "    wave,sr=librosa.load(wav_path,mono=True)\n",
    "    mfccs=librosa.feature.mfcc(y=wave,sr=sr,n_mfcc=HEIGHT)\n",
    "    mfcc=np.pad(mfccs,((0,0),(0,PAD_WIDTH-len(mfccs[0]))),mode='constant')\n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size,path):\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    random.seed(5896)\n",
    "    path=os.path.join(path,'*','*.wav')\n",
    "    waves=gfile.Glob(path)\n",
    "    while True:\n",
    "        random.shuffle(waves)\n",
    "        for wav_path in waves:\n",
    "             _,label=os.path.split(os.path.dirname(wav_path))\n",
    "             X.append(get_mfcc(wav_path))\n",
    "             Y.append(one_hot_encoding(label))\n",
    "             if(len(X)==batch_size):\n",
    "                yield X,Y\n",
    "                X=[]\n",
    "                Y=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input,dropout):\n",
    "    with tf.name_scope('Conv1'):\n",
    "        input_4D = tf.reshape(input,[-1,HEIGHT,WIDTH,1])\n",
    "        w1=tf.Variable(tf.truncated_normal([12,8,1,44],stddev=0.01),name='W')\n",
    "        b1=tf.Variable(tf.zeros([44]),name='B')\n",
    "        conv1=tf.nn.conv2d(input_4D,w1,strides=[1,1,1,1],padding=\"SAME\")\n",
    "        act1=tf.nn.relu(conv1+b1)\n",
    "        drop1=tf.nn.dropout(act1,dropout)\n",
    "        max_pool1=tf.nn.max_pool(drop1,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")\n",
    "        tf.summary.histogram(\"weights\",w1)\n",
    "        tf.summary.histogram(\"biases\",b1)\n",
    "        tf.summary.histogram(\"activations\",act1)\n",
    "        tf.summary.histogram(\"dropouts\",drop1)\n",
    "    with tf.name_scope('Conv2'):\n",
    "        w2=tf.Variable(tf.truncated_normal([6,4,44,44],stddev=0.01),name='W')\n",
    "        b2=tf.Variable(tf.zeros([44]),name='B')\n",
    "        conv2=tf.nn.conv2d(max_pool1,w2,strides=[1,1,1,1],padding=\"SAME\")\n",
    "        act2=tf.nn.relu(conv2+b2)\n",
    "        drop2=tf.nn.dropout(act2,dropout)\n",
    "        tf.summary.histogram(\"weights\",w2)\n",
    "        tf.summary.histogram(\"biases\",b2)\n",
    "        tf.summary.histogram(\"activations\",act2)\n",
    "        tf.summary.histogram(\"dropouts\",drop2)  \n",
    "        \n",
    "        conv_shape=drop2.get_shape()\n",
    "        count=int(conv_shape[1]*conv_shape[2]*conv_shape[3])\n",
    "        flat_output=tf.reshape(drop2,[-1,count])\n",
    "        \n",
    "    with tf.name_scope('FC'):\n",
    "         w3=tf.Variable(tf.truncated_normal([count,NUM_LABELS],stddev=0.01))\n",
    "         b3=tf.Variable(tf.zeros([NUM_LABELS]))\n",
    "         fc=tf.add(tf.matmul(flat_output,w3),b3)\n",
    "         print(\"asdfghjkll\")\n",
    "         print(fc)\n",
    "         tf.summary.histogram(\"weights\",w3)\n",
    "         tf.summary.histogram(\"biases\",b3)\n",
    "         print(\"fc shape\")\n",
    "         print(tf.shape(fc))\n",
    "         \n",
    "         return fc\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    " def main():\n",
    "    tf.reset_default_graph()\n",
    "    sess=tf.Session()\n",
    "    x=tf.placeholder(tf.float32,shape=[None,HEIGHT,WIDTH],name=\"input\")\n",
    "    y=tf.placeholder(tf.float32,shape=[None,NUM_LABELS],name=\"label\")\n",
    "    dropout=tf.placeholder(tf.float32,name='dropout')\n",
    "    logits=get_model(x,dropout)\n",
    "    \n",
    "    with tf.name_scope('loss'):\n",
    "         loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=y))\n",
    "         loss.shape\n",
    "         tf.summary.scalar('loss',loss)\n",
    "         print(\"loss shape\")\n",
    "         print(tf.shape(loss))   \n",
    "    with tf.name_scope(\"train\"):\n",
    "         train_step=tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)\n",
    "         \n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "         predicted=tf.argmax(logits,1)\n",
    "         truth=tf.argmax(y,1)\n",
    "         correct_prediction=tf.equal(predicted,truth)\n",
    "         accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "         print(\"shape accuracy\")\n",
    "         print(tf.shape(accuracy))\n",
    "         confusion_matrix=tf.confusion_matrix(truth,predicted,num_classes=NUM_LABELS)\n",
    "         tf.summary.scalar(\"accuracy\",accuracy)\n",
    "     \n",
    "    summ=tf.summary.merge_all()\n",
    "    print(\"shape summ\")\n",
    "    print(tf.shape(summ))\n",
    "    saver=tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer=tf.summary.FileWriter(LOGDIR)\n",
    "    writer.add_graph(sess.graph)\n",
    "    test_writer=tf.summary.FileWriter(TEST_LOGDIR)\n",
    "    \n",
    "    print(\"Starting Training\\n\")\n",
    "    batch=get_batch(BATCH_SIZE,PATH_TRAIN)\n",
    "    start_time=time.time()\n",
    "    for i in range (1,ITERATIONS+1):\n",
    "        X,Y=next(batch)\n",
    "        if i % EVAL_EVERY == 0:\n",
    "           print(\"accuracy\")\n",
    "           print(accuracy)\n",
    "           [train_accuracy,train_loss,s]=sess.run([accuracy,loss,summ],feed_dict={x:X,y:Y,dropout:0.5})\n",
    "           acc_and_loss=[i,train_loss,train_accuracy*100]\n",
    "           print('Iteration # {}.Train Loss:{:.2f}.Train Acc:{:.0f}%'.format(*acc_and_loss))\n",
    "           writer.add.summary(s,i)\n",
    "            \n",
    "        \n",
    "            \n",
    "        if i%(EVAL_EVERY*20)==0:\n",
    "           train_confusion_matrix=sess.run([confusion_matrix],feed_dict={x:X,y:Y,dropout:0.5})\n",
    "           header=LABEL_TO_INDEX_MAP.keys()\n",
    "           df=pd.DataFrame(np.reshape(train_confusion_matrix,(NUM_LABELS,NUM_LABELS)),index=header)\n",
    "           print(\"\\nConfusion Matrix:\\n{}\\n:\".format(df))\n",
    "           saver=tf.train.Saver()\n",
    "           saver.save(sess,os.path.join(LOGDIR,\"model.ckpt\"),i)\n",
    "            \n",
    "            \n",
    "    sess.run(train_step,feed_dict={x:X,y:Y,dropout:0.5})\n",
    "        \n",
    "    print('\\n Total Training Time:{:0f}seconds\\n'.format(time.time()-start_time))\n",
    "    \n",
    "    batch=get_batch(BATCH_SIZE,PATH_TEST)\n",
    "    total_accuracy=0\n",
    "    for i in range (ITERATIONS_TEST):\n",
    "        X,Y=next(batch,PATH_TEST)\n",
    "        test_accuracy,s=sess.run([accuracy,summ],feed_dict={x:X,y:Y,dropout:1.0})\n",
    "        print('Iteration # {} . Test Accuracy :{:.0.f}%'.format(i+1,test_accuracy*100))\n",
    "        total_accuracy+=(test_accuracy/ITERATIONS_TEST)\n",
    "        test_writer.add_summary(s,i)\n",
    "        \n",
    "        \n",
    "    print('\\n Final Test Accuracy:{:.0f}%'.format(total_accuracy*100))\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asdfghjkll\n",
      "Tensor(\"FC/Add:0\", shape=(?, 6), dtype=float32)\n",
      "fc shape\n",
      "Tensor(\"FC/Shape:0\", shape=(2,), dtype=int32)\n",
      "loss shape\n",
      "Tensor(\"loss/Shape:0\", shape=(0,), dtype=int32)\n",
      "shape accuracy\n",
      "Tensor(\"accuracy/Shape:0\", shape=(0,), dtype=int32)\n",
      "shape summ\n",
      "Tensor(\"Shape:0\", shape=(0,), dtype=int32)\n",
      "Starting Training\n",
      "\n",
      "accuracy\n",
      "Tensor(\"accuracy/Mean:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (20,44) into shape (20)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-469b4743b61b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPATH_TRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-e74ba828442b>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m           \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m           \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m           \u001b[1;33m[\u001b[0m\u001b[0mtrain_accuracy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msumm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m           \u001b[0macc_and_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_accuracy\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m           \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Iteration # {}.Train Loss:{:.2f}.Train Acc:{:.0f}%'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0macc_and_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1102\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1103\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1104\u001b[1;33m             \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1106\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m     \"\"\"\n\u001b[1;32m--> 492\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (20,44) into shape (20)"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    init(PATH_TRAIN)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
